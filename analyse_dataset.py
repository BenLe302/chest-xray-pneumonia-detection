# Analyse avanc√©e du dataset Chest X-Ray (Pneumonia)
# Auteur: Dady Akrou Cyrille
# Email: cyrilledady0501@gmail.com
# Localisation: Trois-Rivi√®res, Canada

import os
import sys
import logging
from pathlib import Path

# Ajouter le r√©pertoire courant au path pour les imports
sys.path.append(str(Path(__file__).parent))

from config import *
from utils import *

def analyser_dataset_avance():
    """
    Analyse compl√®te et avanc√©e du dataset de radiographies thoraciques.
    Utilise les modules de configuration et utilitaires pour une analyse professionnelle.
    """
    # Afficher l'en-t√™te du projet
    print_project_header()
    
    # Configurer le logging
    logger = setup_logging()
    logger.info("D√©but de l'analyse avanc√©e du dataset")
    
    try:
        # Valider la structure du dataset
        print("\nüîç VALIDATION DE LA STRUCTURE DU DATASET")
        print("-" * 60)
        
        validation_results = validate_dataset_structure(DATASET_PATH)
        
        if validation_results['structure_valid']:
            print("‚úÖ Structure du dataset valide")
        else:
            print("‚ùå Probl√®mes d√©tect√©s dans la structure:")
            for issue in validation_results['issues']:
                print(f"  - {issue}")
        
        if validation_results['warnings']:
            print("\n‚ö†Ô∏è Avertissements:")
            for warning in validation_results['warnings']:
                print(f"  - {warning}")
        
        if validation_results['recommendations']:
            print("\nüí° Recommandations:")
            for rec in validation_results['recommendations']:
                print(f"  - {rec}")
        
        # Obtenir les statistiques du dataset
        print("\nüìä STATISTIQUES DU DATASET")
        print("-" * 60)
        
        stats = get_dataset_statistics(DATASET_PATH)
        
        # Afficher les statistiques d√©taill√©es
        for subset in SUBSETS:
            if subset in stats:
                print(f"\nüìÅ Ensemble {subset.upper()}:")
                for class_name in CLASSES:
                    if class_name in stats[subset]:
                        count = stats[subset][class_name]
                        percentage = (count / stats[subset]['total'] * 100) if stats[subset]['total'] > 0 else 0
                        print(f"  {class_name}: {count:,} images ({percentage:.1f}%)")
                print(f"  Total: {stats[subset]['total']:,} images")
        
        print(f"\nüìà Total dataset: {stats['total_dataset']:,} images")
        
        # Calculer et afficher les poids des classes
        class_weights = calculate_class_weights(stats)
        print("\n‚öñÔ∏è Poids calcul√©s pour les classes:")
        for i, class_name in enumerate(CLASSES):
            print(f"  {class_name}: {class_weights[i]:.3f}")
        
        # Analyser les propri√©t√©s des images
        print("\nüñºÔ∏è ANALYSE DES PROPRI√âT√âS D'IMAGES")
        print("-" * 60)
        
        properties = analyze_image_properties(DATASET_PATH, sample_size=100)
        
        if properties['dimensions']:
            widths = [dim[0] for dim in properties['dimensions']]
            heights = [dim[1] for dim in properties['dimensions']]
            ratios = [w/h for w, h in properties['dimensions']]
            file_sizes_mb = [size / (1024 * 1024) for size in properties['file_sizes']]
            
            print(f"Images analys√©es: {len(properties['dimensions']):,}")
            print(f"Largeur moyenne: {np.mean(widths):.0f} px (min: {min(widths)}, max: {max(widths)})")
            print(f"Hauteur moyenne: {np.mean(heights):.0f} px (min: {min(heights)}, max: {max(heights)})")
            print(f"Ratio moyen (L/H): {np.mean(ratios):.2f} (std: {np.std(ratios):.2f})")
            print(f"Taille moyenne: {np.mean(file_sizes_mb):.2f} MB (min: {min(file_sizes_mb):.2f}, max: {max(file_sizes_mb):.2f})")
            print(f"Formats d√©tect√©s: {', '.join(set(properties['formats']))}")
            print(f"Modes couleur: {', '.join(set(properties['color_modes']))}")
        
        return stats, properties, logger
        
    except Exception as e:
        logger.error(f"Erreur lors de l'analyse: {e}")
        raise

def generer_visualisations_avancees(stats, properties, logger):
    """
    G√©n√®re des visualisations avanc√©es et professionnelles du dataset.
    
    Args:
        stats (dict): Statistiques du dataset
        properties (dict): Propri√©t√©s des images
        logger: Logger pour les messages
        
    Returns:
        Path: Chemin vers le fichier de visualisation g√©n√©r√©
    """
    logger.info("G√©n√©ration des visualisations avanc√©es")
    
    try:
        # Cr√©er les visualisations avanc√©es
        create_advanced_visualizations(stats, properties, OUTPUT_PATH)
        
        # Sauvegarder le rapport d'analyse
        report_file = save_analysis_report(stats, properties, OUTPUT_PATH)
        
        logger.info(f"Visualisations sauvegard√©es dans: {OUTPUT_PATH}")
        logger.info(f"Rapport JSON g√©n√©r√©: {report_file}")
        
        return OUTPUT_PATH / 'analyse_avancee_dataset.png'
        
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration des visualisations: {e}")
        raise

def generer_recommandations_ml(stats, properties, logger):
    """
    G√©n√®re des recommandations d√©taill√©es pour le machine learning.
    
    Args:
        stats (dict): Statistiques du dataset
        properties (dict): Propri√©t√©s des images
        logger: Logger pour les messages
    """
    print("\n" + "=" * 80)
    print("RECOMMANDATIONS AVANC√âES POUR LE MACHINE LEARNING")
    print("=" * 80)
    
    # Calculer les m√©triques globales
    total_normal = sum(stats[subset].get('NORMAL', 0) for subset in SUBSETS if subset in stats)
    total_pneumonia = sum(stats[subset].get('PNEUMONIA', 0) for subset in SUBSETS if subset in stats)
    total_images = total_normal + total_pneumonia
    
    # Analyse du d√©s√©quilibre des classes
    if total_pneumonia > total_normal:
        ratio = total_pneumonia / total_normal
        print(f"\nüö® D√âS√âQUILIBRE DES CLASSES CRITIQUE")
        print(f"   Ratio PNEUMONIA:NORMAL = {ratio:.2f}:1")
        print(f"   Impact: Risque √©lev√© de biais vers la classe majoritaire")
        
        print(f"\nüîß Solutions recommand√©es:")
        print(f"   1. Pond√©ration des classes: class_weight={{{0}: {ratio:.2f}, {1}: 1.0}}")
        print(f"   2. SMOTE pour sur-√©chantillonnage de la classe NORMAL")
        print(f"   3. Augmentation de donn√©es cibl√©e sur NORMAL")
        print(f"   4. Sous-√©chantillonnage intelligent de PNEUMONIA")
        print(f"   5. Utilisation de Focal Loss pour g√©rer l'imbalance")
    
    # Analyse de la taille du dataset
    print(f"\nüìä ANALYSE DE LA TAILLE DU DATASET")
    print(f"   Total: {total_images:,} images")
    
    if 'val' in stats and stats['val']['total'] < 100:
        print(f"   ‚ö†Ô∏è Ensemble de validation tr√®s petit: {stats['val']['total']} images")
        print(f"   Recommandation: Redistribuer en 70% train, 20% test, 10% val")
    
    # Recommandations sur les dimensions d'images
    if properties['dimensions']:
        widths = [dim[0] for dim in properties['dimensions']]
        heights = [dim[1] for dim in properties['dimensions']]
        
        print(f"\nüñºÔ∏è PR√âPARATION DES IMAGES")
        print(f"   Dimensions actuelles: {min(widths)}x{min(heights)} √† {max(widths)}x{max(heights)}")
        print(f"   Recommandation: Redimensionner √† {IMAGE_SIZE[0]}x{IMAGE_SIZE[1]} (standard)")
        print(f"   Normalisation: Diviser par 255.0 ou standardisation Z-score")
    
    # Recommandations d'architecture
    print(f"\nüèóÔ∏è ARCHITECTURE DE MOD√àLE RECOMMAND√âE")
    print(f"   1. Transfer Learning avec ResNet50 ou EfficientNet")
    print(f"   2. Fine-tuning progressif (gradual unfreezing)")
    print(f"   3. Dropout (0.3-0.5) pour √©viter l'overfitting")
    print(f"   4. Batch Normalization apr√®s chaque couche convolutionnelle")
    print(f"   5. Learning rate scheduling (ReduceLROnPlateau)")
    
    # M√©triques d'√©valuation
    print(f"\nüìà M√âTRIQUES D'√âVALUATION CRITIQUES")
    print(f"   Primaires: Sensibilit√© (Recall) > 90% pour PNEUMONIA")
    print(f"   Secondaires: Sp√©cificit√© > 80%, F1-Score > 85%")
    print(f"   Globales: AUC-ROC > 0.95, Pr√©cision √©quilibr√©e")
    print(f"   √âviter: Accuracy simple (biais√©e par l'imbalance)")
    
    # Strat√©gie d'augmentation
    print(f"\nüîÑ STRAT√âGIE D'AUGMENTATION DE DONN√âES")
    print(f"   Pour NORMAL (classe minoritaire):")
    print(f"   - Rotation: ¬±{AUGMENTATION_CONFIG['rotation_range']}¬∞")
    print(f"   - Translation: ¬±{AUGMENTATION_CONFIG['width_shift_range']*100}%")
    print(f"   - Zoom: ¬±{AUGMENTATION_CONFIG['zoom_range']*100}%")
    print(f"   - Flip horizontal: {AUGMENTATION_CONFIG['horizontal_flip']}")
    print(f"   - Ajustements de contraste et luminosit√©")
    
    logger.info("Recommandations ML g√©n√©r√©es avec succ√®s")

def generer_rapport_complet():
    """
    G√©n√®re un rapport complet et professionnel d'analyse du dataset.
    
    Returns:
        tuple: (stats, properties, logger) pour utilisation ult√©rieure
    """
    try:
        # Analyse principale
        stats, properties, logger = analyser_dataset_avance()
        
        # G√©n√©rer les visualisations
        viz_path = generer_visualisations_avancees(stats, properties, logger)
        
        # G√©n√©rer les recommandations ML
        generer_recommandations_ml(stats, properties, logger)
        
        # R√©sum√© final
        print("\n" + "=" * 80)
        print("R√âSUM√â DE L'ANALYSE")
        print("=" * 80)
        print(f"‚úÖ Dataset analys√©: {stats['total_dataset']:,} images")
        print(f"‚úÖ Visualisations g√©n√©r√©es: {viz_path}")
        print(f"‚úÖ Rapport JSON sauvegard√©: {OUTPUT_PATH / 'dataset_analysis_report.json'}")
        print(f"‚úÖ Logs d√©taill√©s disponibles dans: {LOGS_PATH}")
        
        logger.info("Analyse compl√®te termin√©e avec succ√®s")
        return stats, properties, logger
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
        if 'logger' in locals():
            logger.error(f"Erreur fatale: {e}")
        raise

if __name__ == "__main__":
    """
    Point d'entr√©e principal pour l'analyse du dataset.
    Ex√©cute une analyse compl√®te et professionnelle.
    """
    try:
        print("üöÄ D√©marrage de l'analyse avanc√©e du dataset Chest X-Ray...")
        
        # Ex√©cuter l'analyse compl√®te
        stats, properties, logger = generer_rapport_complet()
        
        print("\nüéâ Analyse termin√©e avec succ√®s!")
        print(f"üìÅ Consultez les r√©sultats dans: {OUTPUT_PATH}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Analyse interrompue par l'utilisateur")
    except Exception as e:
         print(f"\n‚ùå Erreur fatale: {e}")
         print("Consultez les logs pour plus de d√©tails.")
         sys.exit(1)